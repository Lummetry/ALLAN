The main function of the EY_data algorithms is to create a dataset for the document tagger.

The ey_preprocessing.py script takes care of that functionality. Running the script will:

Generate the dataset for the tagger, and all the other prerequisite files for the tagger and
tagger API endpoint to function. Below find instructions on how to use this script.

#How to use the tagger data generator
1.Add questions to the initial raw ey data, of the form(topics, questions, answers).
  (or any data, new topics, new information for the answers, etc.)

2.Run the ey_preprocesing.py script.
  This will generate a tagger-ready dataset from the format delivered by EY. This dataset
  will not be written to disk, but the details of the outputs will be available in the
  console. This will make the tags visible to you so you can understand which ones are
  desirable, undesirable, and which should be reduced

3.Add tags to exclusions.json and reductions.json. Change the parameters in the config file if needed.

Repeat steps 2,3 until you are happy with the distribution of tags in the dataset.
  
4.Uncomment data.write_to_file() in the __main__ of the file.
  Uncomment data.build_topic_label_map(write_to_disk=True) in the __main__ of the file.

  Comment data.buld_outputs() in the __main__ of the file.
  
  This will write the dataset to Dropbox, make sure not to overwrite any datasets.
  It will also write the topic_label_mapping, as it is needed by the API endpoint to find 
  the best_topic.
  It will also write the topic_idx_mapping, as it is needed by the tagger for training.


#How it works
The inputs, as received by EY are:
  Three corresponding files for each index: t1.txt, q1.txt, a1.txt
  Each contain the topic, questions and answer for the FAQ item.

This script will create a dataset with text, label pairs.
Each text is either: a single line in the questions file, or the entire answers file.
Labels are either: topic_labels, unique to each topic, generated from the topics file
                   regular labels, generated by tokenizing the text of each file

This way there are a lot of regular labels generated, so they need to be denoised/cleaned/trimmed.

This is done in three ways: removing labels that are common to more than a particular percentage of all observations.
                            selecting labels to remove by observing the distribution of the output
                            mapping similar labels to a parent label to reduce the number of labels

All of these methods' parameters can be changed: the first through the OCCURENCE_THRESHOLD in the config file
                                                 the second by adding labels to the exclusions.json file
                                                 the third by adding mappings or labels to existing mappings in reductions.json


#Config parameters
DATA_FOLDER_NAME: the name of the directory where the raw data resides(must be in app folder, under _data)
INDEX_OF_LAST_FILE: number of last file in raw data(so the generator knows when to stop)
OCCURENCE_THRESHOLD: float value between 0-1
                     the minimum percentage of occurence in the dataset a label needs in order to be removed
                      (if a label occurs in more than this threshold of observations, it is removed)
VALIDATION_SET_LENGTH: interger denoting the number of observations randomly selected to make up the validation set
